# IMDB Sentiment Analysis - MLOps Pipeline

A production-ready MLOps pipeline for sentiment analysis of IMDB movie reviews, featuring automated CI/CD, model versioning, and cloud deployment.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [Features](#features)
- [Setup](#setup)
- [Usage](#usage)
- [CI/CD Pipeline](#cicd-pipeline)
- [Deployment](#deployment)
- [Monitoring](#monitoring)
- [Model Metrics](#model-metrics)

## ğŸ¯ Overview

This project implements an end-to-end MLOps pipeline for sentiment analysis using IMDB movie reviews. It demonstrates best practices in:

- **Data Versioning**: DVC for data and model versioning
- **Model Tracking**: MLflow for experiment tracking and model registry
- **CI/CD**: Automated testing, building, and deployment via GitHub Actions
- **Cloud Deployment**: Containerized Flask app on AWS EKS
- **Monitoring**: Prometheus metrics and Grafana dashboards
- **Reproducibility**: Parameterized pipeline with YAML configuration

## ğŸ—ï¸ Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DEVELOPMENT                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Code       â”‚â”€â”€â”€â–¶â”‚   DVC        â”‚â”€â”€â”€â–¶â”‚   MLflow     â”‚      â”‚
â”‚  â”‚   Changes    â”‚    â”‚   Pipeline   â”‚    â”‚   Tracking   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CI/CD PIPELINE                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  GitHub Actions (ubuntu-latest)                          â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Checkout Code                                       â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Install Dependencies                               â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Run DVC Pipeline (dvc repro)                       â”‚   â”‚
â”‚  â”‚  â”‚   â”œâ”€â”€ Data Ingestion                                 â”‚   â”‚
â”‚  â”‚  â”‚   â”œâ”€â”€ Data Preprocessing                             â”‚   â”‚
â”‚  â”‚  â”‚   â”œâ”€â”€ Feature Engineering (TF-IDF)                   â”‚   â”‚
â”‚  â”‚  â”‚   â”œâ”€â”€ Model Training (LogisticRegression)           â”‚   â”‚
â”‚  â”‚  â”‚   â”œâ”€â”€ Model Evaluation                               â”‚   â”‚
â”‚  â”‚  â”‚   â””â”€â”€ Model Registration (MLflow)                   â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Run Tests                                          â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Build Docker Image                                 â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Push to AWS ECR                                    â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Deploy to EKS                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CLOUD INFRASTRUCTURE                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   AWS S3     â”‚    â”‚   AWS ECR    â”‚    â”‚   AWS EKS    â”‚      â”‚
â”‚  â”‚   (Data)     â”‚    â”‚   (Images)   â”‚    â”‚   (Pods)     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                  â”‚                    â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Flask Application (Containerized)                        â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Load Model from MLflow                              â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Serve Predictions                                   â”‚  â”‚
â”‚  â”‚  â””â”€â”€ Expose Prometheus Metrics                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MONITORING                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Prometheus  â”‚â”€â”€â”€â–¶â”‚   Grafana    â”‚    â”‚   MLflow     â”‚      â”‚
â”‚  â”‚  (Metrics)   â”‚    â”‚  (Dashboards)â”‚    â”‚  (Tracking)  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DVC PIPELINE STAGES                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. DATA INGESTION
   Input:  S3/Public URL â†’ data.csv
   Output: data/raw/train.csv, test.csv
   Process: Download, split train/test (80/20)

2. DATA PREPROCESSING
   Input:  data/raw/
   Output: data/interim/train_processed.csv, test_processed.csv
   Process: Text cleaning, lemmatization, stopword removal

3. FEATURE ENGINEERING
   Input:  data/interim/
   Output: data/processed/train_bow.csv, test_bow.csv
          models/vectorizer.pkl
   Process: TF-IDF vectorization (20K features, ngrams 1-3)

4. MODEL BUILDING
   Input:  data/processed/
   Output: models/model.pkl
   Process: Train LogisticRegression (C=10, solver=lbfgs)

5. MODEL EVALUATION
   Input:  models/model.pkl, data/processed/test_bow.csv
   Output: reports/metrics.json, reports/experiment_info.json
   Process: Calculate accuracy, precision, recall, AUC
           Log to MLflow

6. MODEL REGISTRATION
   Input:  reports/experiment_info.json
   Output: MLflow Model Registry
   Process: Register model version, promote to Production
```

### Deployment Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AWS EKS CLUSTER                              â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LoadBalancer Service (flask-app-service)                â”‚  â”‚
â”‚  â”‚  â””â”€â”€ Routes traffic to pods                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Deployment (flask-app) - 2 Replicas                     â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Pod 1: Flask App Container                         â”‚  â”‚
â”‚  â”‚  â”‚   â”œâ”€â”€ Port: 5005                                      â”‚  â”‚
â”‚  â”‚  â”‚   â”œâ”€â”€ Model: MLflow (models:/my_model/Production)    â”‚  â”‚
â”‚  â”‚  â”‚   â”œâ”€â”€ Vectorizer: models/vectorizer.pkl               â”‚  â”‚
â”‚  â”‚  â”‚   â””â”€â”€ Metrics: /metrics (Prometheus)                  â”‚  â”‚
â”‚  â”‚  â””â”€â”€ Pod 2: Flask App Container                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MONITORING STACK                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Prometheus  â”‚â”€â”€â”€â–¶â”‚   Grafana    â”‚    â”‚   EC2        â”‚     â”‚
â”‚  â”‚  (Scrapes    â”‚    â”‚  (Dashboards)â”‚    â”‚  (Hosts      â”‚     â”‚
â”‚  â”‚   /metrics)  â”‚    â”‚              â”‚    â”‚   Monitoring)â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tech Stack

### Core Technologies
- **Python 3.10**: Main programming language
- **Scikit-learn**: Machine learning library
- **Flask**: Web framework for API
- **Gunicorn**: WSGI HTTP server

### MLOps Tools
- **DVC (Data Version Control)**: Data and pipeline versioning
- **MLflow**: Experiment tracking and model registry
- **DAGSHub**: MLflow backend hosting

### Infrastructure
- **Docker**: Containerization
- **AWS ECR**: Container registry
- **AWS EKS**: Kubernetes orchestration
- **AWS S3**: Data storage (DVC remote)

### CI/CD
- **GitHub Actions**: Continuous integration/deployment
- **Kubectl**: Kubernetes deployment

### Monitoring
- **Prometheus**: Metrics collection
- **Grafana**: Visualization and dashboards

## ğŸ“ Project Structure

```
imdb_mlops/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yaml              # CI/CD pipeline configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Raw data (tracked by DVC)
â”‚   â”œâ”€â”€ interim/                 # Preprocessed data (tracked by DVC)
â”‚   â””â”€â”€ processed/               # Feature-engineered data (tracked by DVC)
â”œâ”€â”€ flask_app/
â”‚   â”œâ”€â”€ app.py                   # Flask application
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html          # Web UI
â”‚   â””â”€â”€ requirements.txt        # Flask dependencies
â”œâ”€â”€ models/                      # Trained models (tracked by DVC)
â”‚   â”œâ”€â”€ model.pkl
â”‚   â””â”€â”€ vectorizer.pkl
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for experimentation
â”œâ”€â”€ reports/                     # Evaluation metrics and reports
â”‚   â””â”€â”€ metrics.json
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ promote_model.py        # Model promotion script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ connections/            # AWS connection utilities
â”‚   â”œâ”€â”€ data/                   # Data ingestion and preprocessing
â”‚   â”œâ”€â”€ features/               # Feature engineering
â”‚   â”œâ”€â”€ model/                  # Model training and evaluation
â”‚   â””â”€â”€ logger/                 # Logging utilities
â”œâ”€â”€ tests/                       # Unit and integration tests
â”œâ”€â”€ .dvc/                        # DVC configuration
â”œâ”€â”€ dvc.yaml                     # DVC pipeline definition
â”œâ”€â”€ params.yaml                  # Pipeline parameters
â”œâ”€â”€ deployment.yaml              # Kubernetes deployment config
â”œâ”€â”€ Dockerfile                   # Container image definition
â””â”€â”€ requirements.txt             # Python dependencies
```

## âœ¨ Features

### Model Configuration
- **Algorithm**: Logistic Regression
- **Features**: TF-IDF with Unigrams + Trigrams (1-3 ngrams)
- **Max Features**: 20,000
- **Hyperparameters**:
  - C: 10
  - Penalty: L2
  - Solver: lbfgs
  - Max Iterations: 1000

### Monitoring Metrics
- **Application Metrics**:
  - Request count (by method and endpoint)
  - Request latency (by endpoint)
  - Prediction count (by class)
  - Cost per request (USD)
  - Total accumulated cost (USD)

### Model Metrics
- Accuracy
- Precision
- Recall
- AUC (Area Under Curve)

## ğŸš€ Setup

### Prerequisites
- Python 3.10+
- Conda (recommended)
- Docker
- AWS CLI configured
- kubectl configured
- DVC installed

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd imdb_mlops
```

2. **Create conda environment**
```bash
conda create -n atlas python=3.10
conda activate atlas
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
pip install -r flask_app/requirements.txt
```

4. **Download NLTK data**
```bash
python -c "import nltk; nltk.download('stopwords'); nltk.download('wordnet')"
```

5. **Configure DVC remote (if using S3)**
```bash
dvc remote add -d myremote s3://your-bucket-name/dvc-cache
```

6. **Set up environment variables**
Create a `.env` file:
```bash
DAGSHUB_TOKEN=your_dagshub_token
AWS_ACCESS_KEY_ID=your_aws_key
AWS_SECRET_ACCESS_KEY=your_aws_secret
AWS_BUCKET_NAME=your_bucket_name
AWS_REGION=us-east-1
```

## ğŸ“– Usage

### Running the Pipeline Locally

1. **Run the complete pipeline**
```bash
dvc repro
```

2. **Run specific stages**
```bash
dvc repro data_ingestion
dvc repro model_building
```

3. **Pull data from DVC remote**
```bash
dvc pull
```

4. **Push data to DVC remote**
```bash
dvc push
```

### Running the Flask App Locally

1. **Start the Flask application**
```bash
cd flask_app
python app.py
```

2. **Access the web interface**
```
http://localhost:5005
```

3. **Check metrics endpoint**
```
http://localhost:5005/metrics
```

### Running Tests

```bash
# Run all tests
python -m unittest discover tests

# Run specific test
python -m unittest tests.test_model
python -m unittest tests.test_flask_app
```

## ğŸ”„ CI/CD Pipeline

### Pipeline Stages

1. **Code Checkout**: Clone repository
2. **Environment Setup**: Install Python and dependencies
3. **Pipeline Execution**: Run `dvc repro` (full ML pipeline)
4. **Testing**: Run unit and integration tests
5. **Model Promotion**: Promote model to Production in MLflow
6. **Docker Build**: Build container image
7. **ECR Push**: Push image to AWS ECR
8. **EKS Deployment**: Deploy to Kubernetes cluster

### Triggering CI/CD

The pipeline automatically runs on:
- Push to `main` branch
- Pull requests

### Manual Trigger

You can also trigger manually from GitHub Actions tab.

## ğŸš¢ Deployment

### Prerequisites
- AWS EKS cluster created
- AWS ECR repository created
- kubectl configured for EKS
- AWS credentials configured in GitHub Secrets

### Deployment Process

The CI/CD pipeline automatically:
1. Builds Docker image with latest code
2. Pushes to AWS ECR
3. Updates Kubernetes deployment
4. Restarts pods to pull new image

### Manual Deployment

```bash
# Build and push Docker image
docker build -t your-ecr-repo:latest .
docker push your-ecr-repo:latest

# Deploy to EKS
kubectl apply -f deployment.yaml
kubectl rollout restart deployment flask-app
```

### Accessing the Application

After deployment, get the LoadBalancer URL:
```bash
kubectl get service flask-app-service
```

Access the application at the EXTERNAL-IP:5005

## ğŸ“Š Monitoring

### Prometheus Metrics

The Flask app exposes metrics at `/metrics` endpoint:
- `app_request_count`: Total requests
- `app_request_latency_seconds`: Request latency
- `model_prediction_count`: Prediction counts
- `app_cost_per_request_usd`: Cost per request
- `app_total_cost_usd`: Total cost

### Grafana Dashboards

Configure Prometheus to scrape metrics from:
- Service: `flask-app-service:5005`
- Path: `/metrics`

Create dashboards in Grafana to visualize:
- Request rates and latency
- Prediction distribution
- Cost metrics
- Model performance

### MLflow Tracking

View experiments and models at:
- **DAGSHub MLflow**: `https://dagshub.com/Vinayakmane47/imdb_mlops.mlflow`

Track:
- Model versions
- Training metrics
- Hyperparameters
- Model artifacts

## ğŸ“ˆ Model Metrics

Model evaluation metrics are stored in:
- **Local**: `reports/metrics.json`
- **MLflow**: Logged during evaluation stage

Current model metrics (example):
```json
{
  "accuracy": 0.85,
  "precision": 0.84,
  "recall": 0.86,
  "auc": 0.92
}
```

View metrics:
- **MLflow UI**: Latest experiment run
- **Local file**: `cat reports/metrics.json`
- **Grafana**: If exposed as Prometheus metrics

## ğŸ”§ Configuration

### Pipeline Parameters (`params.yaml`)

```yaml
data_ingestion:
  test_size: 0.20

feature_engineering:
  max_features: 20000
  ngram_range: [1, 3]
  min_df: 1
  max_df: 1.0
  sublinear_tf: true

model_building:
  C: 10
  penalty: l2
  solver: lbfgs
  max_iter: 1000
```

Modify parameters in `params.yaml` and run `dvc repro` to retrain with new settings.

## ğŸ§ª Testing

### Test Structure
- `tests/test_model.py`: Model validation tests
- `tests/test_flask_app.py`: Flask application tests

### Running Tests
```bash
# All tests
python -m unittest discover tests

# Specific test file
python -m unittest tests.test_model
```

## ğŸ“ Data Storage

### Local Development
- Data stored in `data/` directory
- Tracked by DVC (not Git)
- Versioned in DVC cache

### CI/CD
- Data downloaded fresh from source
- Processed on CI runner
- Artifacts not persisted (ephemeral)

### Production
- Models stored in MLflow
- Vectorizer in Docker image
- Data in S3 (DVC remote)

## ğŸ” Security

- **Secrets Management**: GitHub Secrets for sensitive data
- **Kubernetes Secrets**: DAGSHUB_TOKEN stored as K8s secret
- **Environment Variables**: `.env` file (not committed)
- **IAM Roles**: AWS credentials via IAM

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `python -m unittest discover tests`
5. Commit and push
6. Create a pull request

## ğŸ“„ License

See [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- DVC for data versioning
- MLflow for experiment tracking
- DAGSHub for MLflow hosting
- AWS for cloud infrastructure

## ğŸ“§ Contact

For questions or issues, please open an issue on GitHub.

---

**Built with â¤ï¸ using MLOps best practices**
